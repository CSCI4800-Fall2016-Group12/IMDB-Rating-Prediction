{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chris Padgett\n",
    "\n",
    "Greg Peters\n",
    "\n",
    "CSCI 4800 Data Science\n",
    "\n",
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement and Background\n",
    "\n",
    "Wouldn’t it be great if we could determine what movies are good before we see them? There isn’t a simple formula to follow to make a hit movie that everyone enjoys. Maybe a good movie comes from a good story created by movie writers, or maybe it’s from the actors playing the characters, or the director dictating the scene. It could even be a combination of all of these factors. This has led us to ask the question: Can we predict movie review ratings based on the director, cast or writers of a movie?\n",
    "\n",
    "Gaining insight into the questions above is not only useful for the movie viewer, but also for the movie creators. Film is a multi billion dollar industry and is one of the most popular forms of entertainment today. Knowing if any of these data points has an impact on movie ratings could allow creators to make better decisions to make better movies for everyone to enjoy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Sources\n",
    "\n",
    "The data we have decided to use for the project is collected from the International Movie Database imdb.com. IMDB is widely used and updated consistently by many contributors giving us confidence that the data will be accurate.\n",
    "\n",
    "The main data we are interested in for each movie is the critic review score, user review score, writers, directors, and full cast. While we are interested in these data points, other data was taken from the site and might be used as we see fit later in the project. For the list of movies to query from IMDB we used the-numbers.com which is a website that reports information on movie budgets. The list contains over 5,200 movie titles which even after data scrubbing should contain enough data for the machine learning as well as test data for prediction results. Since we scraped the data from IMDB, there will be some cleanup of the data needed up front. Most notably we scraped raw data from two different web pages for each movie in order to get the full cast and writers data needed for our predicion analysis.\n",
    "\n",
    "The code we used to scrape IMDB uses a python library called Scrapy and is locaed in the *Data Collection* folder in our Github repository. Also located in the directory is an iPython notebook with the steps used to collect the data, as well as the raw data json object files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Joining and Cleaning\n",
    "\n",
    "### Load and Parse Raw Data\n",
    "\n",
    "Our raw data that was scraped from imdb.com comes from a main page for each movie as well as a child page that contains the full cast and writers data. As a result, we have data for each movie in two different json files. One file, *imdb_output.json* contains all of the movie data except for the full cast and writers which is stored in *imdb_people.json*. Luckily, IMDB has a unique key for each movie, so joining the data on the *movie_id* will be used and give us high confidence the data is joined correctly. Having this unique key movies also makes it so we know there are no missing or duplicate movies in the dataset.\n",
    "\n",
    "We will be converting cast and writer people into unique *people_id*s since people can have the same name, but be different people. As with the *movie_id*, IMDB also stores people with a unique *people_id* making this task trivial. In addition, we will also be saving a dictionary linking people names to ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5221 movie metadata were loaded!\n",
      "5221 movie people data were loaded!\n",
      "Processed 500 of 5221 movie metadata...\n",
      "Processed 1000 of 5221 movie metadata...\n",
      "Processed 1500 of 5221 movie metadata...\n",
      "Processed 2000 of 5221 movie metadata...\n",
      "Processed 2500 of 5221 movie metadata...\n",
      "Processed 3000 of 5221 movie metadata...\n",
      "Processed 3500 of 5221 movie metadata...\n",
      "Processed 4000 of 5221 movie metadata...\n",
      "Processed 4500 of 5221 movie metadata...\n",
      "Processed 5000 of 5221 movie metadata...\n",
      "Processed 5221 of 5221 movie metadata...\n",
      "Movie metadata processing complete!\n",
      "\n",
      "Creating Name_id to Name dictionary...\n",
      "Saving dictionary...\n",
      "Name_id to Name dictionary created!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import locale\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "locale.setlocale( locale.LC_ALL, 'en_US.UTF-8' ) \n",
    "\n",
    "def parse_price(price):\n",
    "    # eg: u'$237,000,000' --> 237000000\n",
    "    if not price:\n",
    "        return 0\n",
    "    return locale.atoi(re.sub('[^0-9,]', \"\", price))\n",
    "\n",
    "def parse_duration(duration_string):\n",
    "    if not duration_string:\n",
    "        return 0\n",
    "\n",
    "    if \"min\" in duration_string:\n",
    "        if \"h\" in duration_string: # eg: \"2h 49min\"\n",
    "            s = duration_string.split(\"h\")\n",
    "            hours = int(s[0])\n",
    "            if len(s) > 1: # has minute number\n",
    "                if \"min\" in s[1]:\n",
    "                    minutes = int(s[1].strip().split(\"min\")[0])\n",
    "                else:\n",
    "                    minutes = 0\n",
    "            else:\n",
    "                minutes = 0\n",
    "            return 60 * hours + minutes\n",
    "        else: # eg: \"169 min\"\n",
    "            return int(duration_string.split('min')[0])\n",
    "    else:\n",
    "        if \"h\" in duration_string: # eg: \"2h\"\n",
    "            return int(duration_string.split('h')[0].strip()) * 60\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def load_unparsed_movie_metadata():\n",
    "    try:\n",
    "        with open(\"Data Collection/imdb_output.json\", \"r\") as f:\n",
    "            movies = json.load(f)\n",
    "            return movies\n",
    "    except:\n",
    "        print(\"Cannot load the unparsed movie metadata file!\")\n",
    "        return None\n",
    "    \n",
    "def load_unparsed_movie_people():\n",
    "    try:\n",
    "        with open(\"Data Collection/imdb_people.json\", \"r\") as f:\n",
    "            movie_people = json.load(f)\n",
    "            return movie_people\n",
    "    except:\n",
    "        print(\"Cannot load the unparsed movie people metadata file!\")\n",
    "        return None\n",
    "\n",
    "def parse_genres(genres):\n",
    "    if not genres:\n",
    "        return None\n",
    "    return \"|\".join([g.strip() for g in sorted(genres)])\n",
    "\n",
    "def parse_plot_keywords(words):\n",
    "    if not words:\n",
    "        return None\n",
    "    return \"|\".join([w.strip() for w in sorted(words)])\n",
    "\n",
    "def parse_cast_info(cast):\n",
    "    if not cast:\n",
    "        return None\n",
    "    return \"|\".join([c['name_id'].strip() for c in sorted(cast)])\n",
    "\n",
    "def parse_writer_info(writers):\n",
    "    if not writers:\n",
    "        return None\n",
    "    return \"|\".join([w['name_id'].strip() for w in sorted(writers)])\n",
    "\n",
    "def parse_aspect_ratio(ratio_string):\n",
    "    if not ratio_string:\n",
    "        return None\n",
    "    if \":\" in ratio_string:\n",
    "        return float(ratio_string.split(\":\")[0].strip())\n",
    "    else:\n",
    "        return float(re.search('[0-9,.]+', ratio_string).group())\n",
    "\n",
    "def parse_one_movie_metadata(movie, movie_people):\n",
    "    if not movie:\n",
    "        return None\n",
    "\n",
    "    parsed_movie = {}\n",
    "\n",
    "    parsed_movie['movie_imdb_link'] = movie['movie_imdb_link']\n",
    "    parsed_movie['imdb_id'] = movie['imdb_id']\n",
    "    parsed_movie['movie_title'] = movie['movie_title'].encode('utf-8')\n",
    "    parsed_movie['num_voted_users'] = movie['num_voted_users']\n",
    "    parsed_movie['plot_keywords'] = parse_plot_keywords(movie['plot_keywords'])\n",
    "    parsed_movie['num_user_for_reviews'] = movie['num_user_for_reviews']\n",
    "    parsed_movie['language'] = None if movie['language'] is None or len(movie['language']) == 0 else movie['language'][0] # choose the first language\n",
    "    parsed_movie['country'] = None if movie['country'] is None or len(movie['country']) == 0 else movie['country'][0] # choose 1st country\n",
    "    parsed_movie['genres'] = parse_genres(movie['genres'])\n",
    "    parsed_movie['color'] = None if movie['color'] is None or len(movie['color']) == 0 else movie['color'][0]\n",
    "    parsed_movie['gross'] = None if movie['gross'] is None or len(movie['gross']) == 0 else parse_price(movie['gross'][0].strip())\n",
    "    parsed_movie['content_rating'] = None if movie['content_rating'] is None or len(movie['content_rating']) == 0 else movie['content_rating'][0].strip()\n",
    "    parsed_movie['budget'] = None if movie['budget'] is None or len(movie['budget']) == 0 else parse_price(movie['budget'][0].strip())\n",
    "    parsed_movie['title_year'] = None if movie['title_year'] is None else int(movie['title_year'])\n",
    "    parsed_movie['storyline'] = None if movie['storyline'] is None else movie['storyline'].strip().encode('utf-8')\n",
    "    parsed_movie['user_review_score'] = None if movie['user_review_score'] is None or len(movie['user_review_score']) == 0 else float(movie['user_review_score'][0].strip())\n",
    "    parsed_movie['critic_review_score'] = None if movie['critic_review_score'] is None or len(movie['critic_review_score']) == 0 else float(movie['critic_review_score'][0].strip())\n",
    "    parsed_movie['aspect_ratio'] = parse_aspect_ratio(movie['aspect_ratio'])\n",
    "    parsed_movie['num_critic_for_reviews'] = None if movie['num_critic_for_reviews'] is None else movie['num_critic_for_reviews']\n",
    "\n",
    "    # parse movie duration\n",
    "    duration = movie['duration']\n",
    "    if not duration:\n",
    "        parsed_movie['duration'] = None\n",
    "    else:\n",
    "        if len(duration) == 1:\n",
    "            parsed_movie['duration'] = parse_duration(duration[0].strip())\n",
    "        else:\n",
    "            parsed_movie['duration'] = parse_duration(duration[-1].strip())\n",
    "    \n",
    "    # parse director info\n",
    "    director_info = movie['director_info']\n",
    "    if not director_info:\n",
    "        parsed_movie['director_name_id'] = None\n",
    "    else:\n",
    "        parsed_movie['director_name_id'] = director_info['name_id'].encode('utf-8')\n",
    "        \n",
    "    # parse cast and writer info\n",
    "    for movie_crew in movie_people:\n",
    "        if parsed_movie['imdb_id'] == movie_crew['imdb_id']:\n",
    "            parsed_movie['cast_info'] = parse_cast_info(movie_crew['cast_info'])\n",
    "            parsed_movie['writer_info'] = parse_writer_info(movie_crew['writer_info'])\n",
    "            break\n",
    "\n",
    "    return parsed_movie\n",
    "\n",
    "def parse_all_movies():\n",
    "    movies = load_unparsed_movie_metadata()\n",
    "    total_movies = len(movies)\n",
    "    print(\"{} movie metadata were loaded!\".format(total_movies))\n",
    "\n",
    "    movie_people = load_unparsed_movie_people()\n",
    "    total_movie_people = len(movie_people)\n",
    "    print(\"{} movie people data were loaded!\".format(total_movie_people))\n",
    "    \n",
    "    with open(\"Datasets/movie_metadata_all.csv\", \"w\") as f:\n",
    "        header_was_written = False\n",
    "        for i, movie in enumerate(movies):\n",
    "            parsed_movie = parse_one_movie_metadata(movie, movie_people)\n",
    "            w = csv.DictWriter(f, parsed_movie.keys())\n",
    "            if not header_was_written:\n",
    "                w.writeheader()\n",
    "                header_was_written = True\n",
    "\n",
    "            w.writerow(parsed_movie)\n",
    "            if (i+1) % 500 == 0 or i+1 == total_movies:\n",
    "                print(\"Processed {} of {} movie metadata...\".format(i+1, total_movies))\n",
    "            \n",
    "    print(\"Movie metadata processing complete!\")\n",
    "\n",
    "def create_people_name_dict():\n",
    "    print(\"\\nCreating Name_id to Name dictionary...\")\n",
    "    movie_people = load_unparsed_movie_people()\n",
    "    \n",
    "    people_id_dict = {}\n",
    "    for movie in movie_people:\n",
    "        for actor in movie['cast_info']:\n",
    "            people_id_dict[actor['name_id']] = actor['actor_name'].replace('\\n', '')\n",
    "            \n",
    "        for writer in movie['writer_info']:\n",
    "            people_id_dict[writer['name_id']] = writer['writer_name'].replace('\\n', '')\n",
    "    \n",
    "    print(\"Saving dictionary...\")\n",
    "    with open('Datasets/people_id_dictionary.pickle', 'wb') as handle:\n",
    "        pickle.dump(people_id_dict, handle)\n",
    "        \n",
    "    print(\"Name_id to Name dictionary created!\")\n",
    "    \n",
    "parse_all_movies()\n",
    "create_people_name_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Now that we have parsed the data we will need to remove any movies that are missing data for the fields in which we are interested. Namely, we will need to remove any movie that is missing the cast info or writer info as they are the focus of the machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies missing cast_info: 28\n",
      "Movies missing writer_info: 335\n",
      "Removing movies missing data...\n",
      "Movies remaining: 4886\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dirty_movie_data = pd.read_csv(\"Datasets/movie_metadata_all.csv\")\n",
    "required_fields = {'cast_info': 0, 'writer_info': 0}\n",
    "\n",
    "def clean_movies(dirty_movie_data, required_fields):\n",
    "    missing_data = []\n",
    "    for i, movie in dirty_movie_data.iterrows():\n",
    "        for req_field in required_fields:\n",
    "            if pd.isnull(movie[req_field]):\n",
    "                missing_data.append(movie['imdb_id'])\n",
    "                required_fields[req_field] = required_fields[req_field] + 1\n",
    "            \n",
    "    for req_field in required_fields:\n",
    "        print('Movies missing {}: {}'.format(req_field, required_fields[req_field]))\n",
    "    \n",
    "    missing_data = set(missing_data)\n",
    "    print('Removing movies missing data...')\n",
    "    return dirty_movie_data[~dirty_movie_data['imdb_id'].isin(missing_data)]\n",
    "\n",
    "clean_movie_data = clean_movies(dirty_movie_data, required_fields)\n",
    "print('Movies remaining: {}'.format(len(clean_movie_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Critic and User Datasets\n",
    "\n",
    "Since we are investigating two different ratings, critic and user, we will create two data sets; one for each. When making these datasets we will also need to remove movies that are missing the critic rating or user rating in the respective datasets. Also, if the number of contributed critic ratings or user ratings is too low we will also remove it. This is aimed at making the data more accurate by having a better sample size for the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critic Dataset - missing critic_review_score: 1144\n",
      "Critic Dataset - not enough reviews (<10): 303\n",
      "Removing movies from critic dataset...\n",
      "Full Critic Dataset contains 3717 movies\n",
      "\n",
      "User Dataset - missing user_review_score: 39\n",
      "User Dataset - not enough reviews (<100): 100\n",
      "Removing movies from user dataset...\n",
      "Full User Dataset contains 4786 movies\n"
     ]
    }
   ],
   "source": [
    "min_required_critic_reviews = 10\n",
    "min_required_user_reviews = 100\n",
    "\n",
    "def create_critic_dataset(clean_movie_data, min_required_critic_reviews):\n",
    "    dirty_critic_data = {'critic_review_score': [], 'num_critic_for_reviews': []}\n",
    "    for i, movie in clean_movie_data.iterrows():\n",
    "        if pd.isnull(movie['critic_review_score']):\n",
    "            dirty_critic_data['critic_review_score'].append(movie['imdb_id'])\n",
    "            \n",
    "        if pd.isnull(movie['num_critic_for_reviews']) or movie['num_critic_for_reviews'] < min_required_critic_reviews:\n",
    "            dirty_critic_data['num_critic_for_reviews'].append(movie['imdb_id'])\n",
    "            \n",
    "    print('Critic Dataset - missing critic_review_score: {}'.format(len(dirty_critic_data['critic_review_score'])))\n",
    "    print('Critic Dataset - not enough reviews (<{}): {}'.format(min_required_critic_reviews, len(dirty_critic_data['num_critic_for_reviews'])))\n",
    "    \n",
    "    critic_removal_list = set(dirty_critic_data['critic_review_score'] + dirty_critic_data['num_critic_for_reviews'])\n",
    "    print('Removing movies from critic dataset...')\n",
    "    return clean_movie_data[~clean_movie_data['imdb_id'].isin(critic_removal_list)]\n",
    "\n",
    "def create_user_dataset(clean_movie_data, min_required_user_reviews):\n",
    "    dirty_user_data = {'user_review_score': [], 'num_voted_users': []}\n",
    "    for i, movie in clean_movie_data.iterrows():\n",
    "        if pd.isnull(movie['user_review_score']):\n",
    "            dirty_user_data['user_review_score'].append(movie['imdb_id'])\n",
    "            \n",
    "        if pd.isnull(movie['num_voted_users']) or movie['num_voted_users'] < min_required_user_reviews:\n",
    "            dirty_user_data['num_voted_users'].append(movie['imdb_id'])\n",
    "            \n",
    "    print('User Dataset - missing user_review_score: {}'.format(len(dirty_user_data['user_review_score'])))\n",
    "    print('User Dataset - not enough reviews (<{}): {}'.format(min_required_user_reviews, len(dirty_user_data['num_voted_users'])))\n",
    "    \n",
    "    user_removal_list = set(dirty_user_data['user_review_score'] + dirty_user_data['num_voted_users'])\n",
    "    print('Removing movies from user dataset...')\n",
    "    return clean_movie_data[~clean_movie_data['imdb_id'].isin(user_removal_list)]\n",
    "    \n",
    "full_critic_dataset = create_critic_dataset(clean_movie_data, min_required_critic_reviews)\n",
    "print('Full Critic Dataset contains {} movies\\n'.format(len(full_critic_dataset)))\n",
    "full_critic_dataset.to_csv('Datasets/full_critic_dataset.csv')\n",
    "\n",
    "full_user_dataset = create_user_dataset(clean_movie_data, min_required_user_reviews)\n",
    "print('Full User Dataset contains {} movies'.format(len(full_user_dataset)))\n",
    "full_user_dataset.to_csv('Datasets/full_user_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide Datasets into Learn and Test Data\n",
    "\n",
    "We will be dividing the datasets into Learning and Test Datasets. The Learning Dataset will be used in the machine learning prediction algorithm and the Test Dataset will be used to test the prediction algorithm guess for accuracy. 90% of the full datasets will be used in the Learning Dataset and 10% for the Test Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets/full_critic_dataset.csv contains 3717 movies\n",
      "Critic Learning Dataset size: 3345\n",
      "Critic Testing Dataset size: 372\n",
      "\n",
      "Datasets/full_user_dataset.csv contains 4786 movies\n",
      "User Learning Dataset size: 4307\n",
      "User Testing Dataset size: 479\n"
     ]
    }
   ],
   "source": [
    "def divide_dataset_learn_test(dataset_path, learn_path, test_path, learn_mod):\n",
    "    ds = pd.read_csv(dataset_path)\n",
    "    print('{} contains {} movies'.format(dataset_path, len(ds)))\n",
    "    \n",
    "    learn_ds = pd.DataFrame([row for i, row in ds.iterrows() if (i % learn_mod) != 0])\n",
    "    test_ds = pd.DataFrame([row for i, row in ds.iterrows() if (i % learn_mod) == 0])\n",
    "                    \n",
    "    learn_ds.to_csv(learn_path)\n",
    "    test_ds.to_csv(test_path)\n",
    "            \n",
    "learn_mod = 10\n",
    "divide_dataset_learn_test('Datasets/full_critic_dataset.csv', 'Datasets/learn_critic_dataset.csv', 'Datasets/test_critic_dataset.csv', learn_mod)\n",
    "learn_critic_dataset = pd.read_csv('Datasets/learn_critic_dataset.csv')\n",
    "test_critic_dataset = pd.read_csv('Datasets/test_critic_dataset.csv')\n",
    "print('Critic Learning Dataset size: {}'.format(len(learn_critic_dataset)))\n",
    "print('Critic Testing Dataset size: {}\\n'.format(len(test_critic_dataset)))\n",
    "\n",
    "divide_dataset_learn_test('Datasets/full_user_dataset.csv', 'Datasets/learn_user_dataset.csv', 'Datasets/test_user_dataset.csv', learn_mod)\n",
    "learn_user_dataset = pd.read_csv('Datasets/learn_user_dataset.csv')\n",
    "test_user_dataset = pd.read_csv('Datasets/test_user_dataset.csv')\n",
    "print('User Learning Dataset size: {}'.format(len(learn_user_dataset)))\n",
    "print('User Testing Dataset size: {}'.format(len(test_user_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Approach\n",
    "\n",
    "Describe what analysis you are doing: This will probably comprise:\n",
    "* Featurization: Explain how you generated features from the raw data, e.g., thresholding to produce binary features, binning, tf-idf, multinomial -> multiple binary features (one-hot encoding). Describe any value transformations you did, e.g., histogram normalization.\n",
    "* Modeling: Which machine learning models did you try? Which do you plan to try in the future?\n",
    "* Performance measurement: How will you evaluate your model and improve featurization, etc.\n",
    "\n",
    "## Notes on coding:\n",
    "* From parsed data we make four tables (technically 4 for learning, and 4 for testing the predictions):\n",
    "    * User reviews cast\n",
    "        * each row is a movie\n",
    "        * one column for movie_id\n",
    "        * one column for user_review_score\n",
    "        * one column per person_id, if the person is a cast member in the movie true, if not in the movie then false\n",
    "    * User reviews writers\n",
    "        * each row is a movie\n",
    "        * one column for movie_id\n",
    "        * one column for user_review_score\n",
    "        * one column per person_id, if the person is a writer for the movie true, if not in the movie then false\n",
    "    * Critic reviews cast\n",
    "        * each row is a movie\n",
    "        * one column for movie_id\n",
    "        * one column for critic_review_score\n",
    "        * one column per person_id, if the person is a cast member in the movie true, if not in the movie then false\n",
    "    * Critic reviews writers\n",
    "        * each row is a movie\n",
    "        * one column for movie_id\n",
    "        * one column for critic_review_score\n",
    "        * one column per person_id, if the person is a writer for the movie true, if not in the movie then false\n",
    "* Number of columns is really big, one column for each person in our person dictionary, so we write a row at a time for memory performance reasons, unfortunately this takes a long time for each movie\n",
    "* Each column is needed for the lin regression, especially for testing where we could use any person as a writer or cast member\n",
    "* Code in this section is primarily to show us changing the parsed data into a data format that the linear regression can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating linear regression dataset for: 'user_review_score' and 'cast_info'\n",
      "Writing dataset to 'Datasets/lin_reg/test_user_cast_lin_reg.csv'\n",
      "Writing...\n",
      "More writing...\n",
      "  Processed 1 of 1 movies...\n",
      "Finished writing file Datasets/lin_reg/test_user_cast_lin_reg.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Datasets for machine learning\n",
    "learn_user_movdata = pd.read_csv('Datasets/learn_user_dataset.csv')\n",
    "learn_critic_movdata = pd.read_csv('Datasets/learn_critic_dataset.csv')\n",
    "\n",
    "# Datasets for testing predictions\n",
    "test_user_movdata = pd.read_csv('Datasets/test_user_dataset.csv')\n",
    "test_critic_movdata = pd.read_csv('Datasets/test_critic_dataset.csv')\n",
    "\n",
    "# Load people_id dictionary\n",
    "with open('Datasets/people_id_dictionary.pickle', 'rb') as handle:\n",
    "    people_id_dict = pickle.load(handle)\n",
    "\n",
    "    \n",
    "# Function to get a column from a dataset\n",
    "def get_col(colname, dataset):\n",
    "    return [x[colname] for i, x in dataset.iterrows()]\n",
    "\n",
    "\n",
    "# Function to get a list of people from a people string\n",
    "def get_people_list(people_string):\n",
    "    return [name for name in people_string.split(\"|\")]\n",
    "\n",
    "\n",
    "# Function to get person name from person_id\n",
    "def get_person_name(people_dict, name_id):\n",
    "    return people_dict[name_id]\n",
    "\n",
    "\n",
    "# Function to create linear regression dataset\n",
    "def create_lin_reg_data(mov_dataset, people_dict, review_tag, person_tag, out_dataset_file_path):\n",
    "    print(\"Creating linear regression dataset for: '{}' and '{}'\".format(review_tag, person_tag))\n",
    "    print(\"Writing dataset to '{}'\".format(out_dataset_file_path))\n",
    "    total_movies = len(mov_dataset)\n",
    "    \n",
    "    with open(out_dataset_file_path, \"w\") as f:\n",
    "        header_was_written = False\n",
    "        movie_row = {}\n",
    "        for i, movie in mov_dataset.iterrows():\n",
    "            movie_row['imdb_id'] = movie['imdb_id']\n",
    "            movie_row[review_tag] = movie[review_tag]\n",
    "\n",
    "            movie_people = get_people_list(movie[person_tag])\n",
    "            for person_id in people_dict.iterkeys():\n",
    "                movie_row[person_id] = 1 if person_id in movie_people else 0\n",
    "\n",
    "            w = csv.DictWriter(f, movie_row.keys())\n",
    "            if not header_was_written:\n",
    "                w.writeheader()\n",
    "                header_was_written = True\n",
    "\n",
    "            w.writerow(movie_row)\n",
    "            if (i+1) % 500 == 0 or i+1 == total_movies:\n",
    "                print(\"  Processed {} of {} movies...\".format(i+1, total_movies))\n",
    "                \n",
    "    print(\"Finished writing file {}\\n\".format(out_dataset_file_path))\n",
    "\n",
    "\n",
    "# Test Datasets\n",
    "test_user_cast_data_file = 'Datasets/lin_reg/test_user_cast_lin_reg.csv'\n",
    "create_lin_reg_data(test_user_movdata, people_id_dict, 'user_review_score', 'cast_info', test_user_cast_data_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Results\n",
    "\n",
    "Summarize the results you have so far:\n",
    "* Define suitable performance measures for your problem. Explain why they make sense, and what other measures you considered.\n",
    "* Give the results. These might include accuracy scores, ROC plots and AUC, or precision/recall plots, or results of hypothesis tests.\n",
    "* Describe any tuning that you did.\n",
    "* Explain any hypothesis tests you did. Be explicit about the null and alternative hypothesis. Be very clear about the test you used and how you used it. Include all the experiment details (between/within-subjects, degrees-of-freedom, etc). Be frugal with tests. Do not try many tests and report the best results.\n",
    "* Use graphics! Please use visual presentation whenever possible. The next best option is a table. Try to avoid \"inlining\" important results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis, any Obstacles\n",
    "\n",
    "Describe the final analysis you plan to do:\n",
    "* Scale: how much data will you use?\n",
    "* Model complexity: What complexity of models will you use, this is relevant for models like clustering, factor models, Random Forests, etc.\n",
    "* What tools will you use?\n",
    "* Estimate of processing time? You should be able to form an estimate of how much time you need on your chosen tools.\n",
    "\n",
    "and outline any obstacles you foresee."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
